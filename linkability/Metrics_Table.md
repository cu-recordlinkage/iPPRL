## Metrics Table 

### Intrinsic Data Quality Measures                                                                                                                                                                                                                           

| Name                                                    | Description                                                                                                                                                                                       |
|:--------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Linkage Variable Availability                           | For all potential linkage data elements listed in the data dictionary, list availability either as not available, available as a data element, or available as a computed data element.           |
| Data Validity                                           | Percent of rows that exceed simple validation rules that test for extreme upper and lower limits of valid values, and impossible values. <ul><li> Date of birth on future date </li> <li> Invalid Social Security Number </li><li> Impossible values (outside of permissible value range or list permissible values)  </li> <li> Data model conformant check </li>                                                                                                                                                                   |

### Distributional Measures                                 
| Name                                                    | Description                                                                                                                                                                                       |
|:--------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Data Completeness: Missing Data Ratio (MDR)             | <a href="https://www.codecogs.com/eqnedit.php?latex=MDR_i&space;=&space;\frac{Number\,&space;of\,&space;records\,&space;with\,&space;missing\,&space;value\,&space;in\,&space;Variable_i}&space;{Total\,&space;number\,&space;of\,&space;records\,&space;in\,&space;Variable_i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?MDR_i&space;=&space;\frac{Number\,&space;of\,&space;records\,&space;with\,&space;missing\,&space;value\,&space;in\,&space;Variable_i}&space;{Total\,&space;number\,&space;of\,&space;records\,&space;in\,&space;Variable_i}" title="MDR_i = \frac{Number\, of\, records\, with\, missing\, value\, in\, Variable_i} {Total\, number\, of\, records\, in\, Variable_i}" /></a>|
| Distinct Values Ratio (DVR)                             | The number of distinct values compared to all records with non-missing values. <br><br> <a href="https://www.codecogs.com/eqnedit.php?latex=DVR_i&space;=&space;\frac{Number\,&space;of\,&space;distinct\,&space;values\,&space;for\,&space;Variable_i}&space;{Number\,&space;of\,&space;records\,&space;in\,&space;Variable_i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?DVR_i&space;=&space;\frac{Number\,&space;of\,&space;distinct\,&space;values\,&space;for\,&space;Variable_i}&space;{Number\,&space;of\,&space;records\,&space;in\,&space;Variable_i}" title="DVR_i = \frac{Number\, of\, distinct\, values\, for\, Variable_i} {Number\, of\, records\, in\, Variable_i}" /></a>                                                                                                                   |
| Average, Max, and Standard Deviation of Group Size (GS) | <a href="https://www.codecogs.com/eqnedit.php?latex=GS_i,_j&space;=&space;{Number\,&space;of\,&space;rows\,&space;for\,&space;Variable_i,&space;Value_j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?GS_i,_j&space;=&space;{Number\,&space;of\,&space;rows\,&space;for\,&space;Variable_i,&space;Value_j}" title="GS_i,_j = {Number\, of\, rows\, for\, Variable_i, Value_j}" /></a>                                                                                                                                                     |
|                                                         | <a href="https://www.codecogs.com/eqnedit.php?latex=Min(GS_i)&space;=&space;{argmin_j(GS_i,_j)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Min(GS_i)&space;=&space;{argmin_j(GS_i,_j)}" title="Min(GS_i) = {argmin_j(GS_i,_j)}" /></a>                                                                                                                                                                         |
|                                                         | <a href="https://www.codecogs.com/eqnedit.php?latex=Max(GS_i)&space;=&space;{argmax_j(GS_i,_j)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Max(GS_i)&space;=&space;{argmax_j(GS_i,_j)}" title="Max(GS_i) = {argmax_j(GS_i,_j)}" /></a>                                                                                                                                                                       |
|                                                         | <a href="https://www.codecogs.com/eqnedit.php?latex=Mean(GS_i)&space;=&space;{mean(GS_i,_j)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Mean(GS_i)&space;=&space;{mean(GS_i,_j)}" title="Mean(GS_i) = {mean(GS_i,_j)}" /></a>                                                                                                                                                                         |
|                                                         | <a href="https://www.codecogs.com/eqnedit.php?latex=SD(GS_i)&space;=&space;{stddev(GS_i,_j)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?SD(GS_i)&space;=&space;{stddev(GS_i,_j)}" title="SD(GS_i) = {stddev(GS_i,_j)}" /></a>                                                                                                                                                                        |
| Shannon Entropy (H)                                     | SE is the mutual information of an LV with itself. For Variable x with values i: <br> <br> <a href="https://www.codecogs.com/eqnedit.php?latex=H_x&space;=&space;-\sum_{i=1}^{N}P(i)&space;\,&space;log_2&space;\,P(i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?H_x&space;=&space;-\sum_{i=1}^{N}P(i)&space;\,&space;log_2&space;\,P(i)" title="H_x = -\sum_{i=1}^{N}P(i) \, log_2 \,P(i)" /></a>  <br><br>where N is the number of unique values that Variable x may take on. |                                                                                                                                                                         |
| Joint Entropy (JE)                                      | Joint Entropy (JE) is high (good) when variable values are independent and low (bad) when variables are highly correlated. For Variable x with values I and Variable y with values J:  <br><br> <a href="https://www.codecogs.com/eqnedit.php?latex=JE_x,_y=\sum_{i=I}\sum_{j=J}&space;P(i,j)&space;\,&space;log_2\,&space;P(i,j)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?JE_x,_y=\sum_{i=I}\sum_{j=J}&space;P(i,j)&space;\,&space;log_2\,&space;P(i,j)" title="JE_x,_y=\sum_{i=I}\sum_{j=J} P(i,j) \, log_2\, P(i,j)" /></a>           |
| Theoretical Maximum Entropy (TME)                       | Theoretical maximum entropy is the maximum entropy possible for a given Variable x with values I. Maximum entropy is reached when each possible i  occurs with the same probability in Variable x <br> <br> <a href="https://www.codecogs.com/eqnedit.php?latex=TME_x=&space;-log_2(\frac{1}{N})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?TME_x=&space;-log_2(\frac{1}{N})" title="TME_x= -log_2(\frac{1}{N})" /></a>  |
|                                                         | <a href="https://www.codecogs.com/eqnedit.php?latex=\rightarrow&space;\,\,&space;-N*(\frac{1}{N}*log2(\frac{1}{N}))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\rightarrow&space;\,\,&space;-N*(\frac{1}{N}*log2(\frac{1}{N}))" title="\rightarrow \,\, -N*(\frac{1}{N}*log2(\frac{1}{N}))" /></a>                                                                                                                                                                                  |
|                                                         | <a href="https://www.codecogs.com/eqnedit.php?latex=\rightarrow&space;\,\,&space;-log2(\frac{1}{N})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\rightarrow&space;\,\,&space;-log2(\frac{1}{N})" title="\rightarrow \,\, -log2(\frac{1}{N})" /></a>                                                                                                                                                                                        |
| Theoretical Maximum Joint Entropy (TMJE)                | Formula TBD                                                                                                                                                                                       |
| Percentage of Theoretical Maximum Entropy  (PTME)       | Percentage of the TME reached by the SE for this variable: <br> <br> <a href="https://www.codecogs.com/eqnedit.php?latex=PTME_x&space;=&space;\frac{H_x}{TME_x}*100" target="_blank"><img src="https://latex.codecogs.com/gif.latex?PTME_x&space;=&space;\frac{H_x}{TME_x}*100" title="PTME_x = \frac{H_x}{TME_x}*100" /></a>           |                                                                                                                                                                      |
| Average Token Frequency (ATF)                           | The average frequency of tokens for this Variable: <br> <br>  <br> <br> where V is the number of rows in this variable.
| Mutual Information (MI)                                 | The mutual information between two variables describes how much information can be learned about one variable from observing the other.  <br> <br>  <a href="https://www.codecogs.com/eqnedit.php?latex=MI(V_x,V_y)&space;=&space;H(V_x)&space;&plus;&space;H(V_y)&space;-&space;JE(V_x,V_y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?MI(V_x,V_y)&space;=&space;H(V_x)&space;&plus;&space;H(V_y)&space;-&space;JE(V_x,V_y)" title="MI(V_x,V_y) = H(V_x) + H(V_y) - JE(V_x,V_y)" /></a>                                                        |


### Metric Research:

#### Shannon’s Entropy
Measures the information contained within a variable. A variable will have low entropy when it is single-valued. The variable will have the highest entropy when each observation of the variable is unique. When the set of unique values is known (like it is for our linkage columns), then the TME occurs where each unique value is uniformly probable.

#### Theoretical Maximum Entropy
The TME, or maximum entropy distribution occurs when entropy is highest. For a discrete-valued column (like our linkage columns), the uniform distribution maximizes the entropy[3, eq. 8.19], as it corresponds to having the least information about the column.

#### Mutual Information
Measures the information shared between two variables. Two variables that are not correlated with have a low mutual information, because you do not learn much about one variable from observing the other. Two variables that are correlated will have high mutual information, because you can learn information about one variable from observing the other [2, p.4]. 

#### Joint Entropy
Generalization of Entropy to multiple-valued variables[1, p.3], whereas Mutual Information actually measures the shared knowledge between two variables.

#### Actions Needed
Add shannon entropy/maximum entropy relation discussion, and proposed metric for PTME * DVR

#### References:

[1] Entropy and Mutual Information, UMass Amherst https://people.cs.umass.edu/~elm/Teaching/Docs/mutInf.pdf

[2] Review of Basic Probability, UMass Amherst https://people.cs.umass.edu/~elm/Teaching/Docs/probReview.pdf

[3] Lecture 8: Information Theory and Maximum Entropy, Morais,M. 
http://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes08_infotheory.pdf
